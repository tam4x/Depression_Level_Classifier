{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the path to the directory where your module is located\n",
    "module_path = os.path.abspath(os.path.join('..', 'C:\\\\Users\\\\pier1\\\\OneDrive\\\\Desktop\\\\uni\\\\Master\\\\2.Semester\\\\Machine Learning (WIWI)\\\\Project\\\\Data for depression\\\\'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Now you can import your module\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from NN import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-9.0\n",
      "ID_1          203\n",
      "ID_2          203\n",
      "group_id      203\n",
      "SEX           203\n",
      "AGE           203\n",
      "             ... \n",
      "FEATURE_51    203\n",
      "FEATURE_52    203\n",
      "FEATURE_53    203\n",
      "FEATURE_54    203\n",
      "FEATURE_55    203\n",
      "Length: 65, dtype: int64\n",
      "-8.0\n",
      "ID_1          149\n",
      "ID_2          149\n",
      "group_id      149\n",
      "SEX           149\n",
      "AGE           149\n",
      "             ... \n",
      "FEATURE_51    149\n",
      "FEATURE_52    149\n",
      "FEATURE_53    149\n",
      "FEATURE_54    149\n",
      "FEATURE_55    149\n",
      "Length: 65, dtype: int64\n",
      "-6.0\n",
      "ID_1          146\n",
      "ID_2          146\n",
      "group_id      146\n",
      "SEX           146\n",
      "AGE           146\n",
      "             ... \n",
      "FEATURE_51    146\n",
      "FEATURE_52    146\n",
      "FEATURE_53    146\n",
      "FEATURE_54    146\n",
      "FEATURE_55    146\n",
      "Length: 65, dtype: int64\n",
      "-3.0\n",
      "ID_1          403\n",
      "ID_2          403\n",
      "group_id      403\n",
      "SEX           403\n",
      "AGE           403\n",
      "             ... \n",
      "FEATURE_51    403\n",
      "FEATURE_52    403\n",
      "FEATURE_53    403\n",
      "FEATURE_54    403\n",
      "FEATURE_55    403\n",
      "Length: 65, dtype: int64\n",
      "-2.0\n",
      "ID_1          460\n",
      "ID_2          460\n",
      "group_id      460\n",
      "SEX           460\n",
      "AGE           460\n",
      "             ... \n",
      "FEATURE_51    460\n",
      "FEATURE_52    460\n",
      "FEATURE_53    460\n",
      "FEATURE_54    460\n",
      "FEATURE_55    460\n",
      "Length: 65, dtype: int64\n",
      "-1.0\n",
      "ID_1          3318\n",
      "ID_2          3318\n",
      "group_id      3318\n",
      "SEX           3318\n",
      "AGE           3318\n",
      "              ... \n",
      "FEATURE_51    3318\n",
      "FEATURE_52    3318\n",
      "FEATURE_53    3318\n",
      "FEATURE_54    3318\n",
      "FEATURE_55    3318\n",
      "Length: 65, dtype: int64\n",
      "0.0\n",
      "ID_1          5790\n",
      "ID_2          5790\n",
      "group_id      5790\n",
      "SEX           5790\n",
      "AGE           5790\n",
      "              ... \n",
      "FEATURE_51    5790\n",
      "FEATURE_52    5790\n",
      "FEATURE_53    5790\n",
      "FEATURE_54    5790\n",
      "FEATURE_55    5790\n",
      "Length: 65, dtype: int64\n",
      "1.0\n",
      "ID_1          3318\n",
      "ID_2          3318\n",
      "group_id      3318\n",
      "SEX           3318\n",
      "AGE           3318\n",
      "              ... \n",
      "FEATURE_51    3318\n",
      "FEATURE_52    3318\n",
      "FEATURE_53    3318\n",
      "FEATURE_54    3318\n",
      "FEATURE_55    3318\n",
      "Length: 65, dtype: int64\n",
      "2.0\n",
      "ID_1          460\n",
      "ID_2          460\n",
      "group_id      460\n",
      "SEX           460\n",
      "AGE           460\n",
      "             ... \n",
      "FEATURE_51    460\n",
      "FEATURE_52    460\n",
      "FEATURE_53    460\n",
      "FEATURE_54    460\n",
      "FEATURE_55    460\n",
      "Length: 65, dtype: int64\n",
      "3.0\n",
      "ID_1          403\n",
      "ID_2          403\n",
      "group_id      403\n",
      "SEX           403\n",
      "AGE           403\n",
      "             ... \n",
      "FEATURE_51    403\n",
      "FEATURE_52    403\n",
      "FEATURE_53    403\n",
      "FEATURE_54    403\n",
      "FEATURE_55    403\n",
      "Length: 65, dtype: int64\n",
      "6.0\n",
      "ID_1          146\n",
      "ID_2          146\n",
      "group_id      146\n",
      "SEX           146\n",
      "AGE           146\n",
      "             ... \n",
      "FEATURE_51    146\n",
      "FEATURE_52    146\n",
      "FEATURE_53    146\n",
      "FEATURE_54    146\n",
      "FEATURE_55    146\n",
      "Length: 65, dtype: int64\n",
      "8.0\n",
      "ID_1          149\n",
      "ID_2          149\n",
      "group_id      149\n",
      "SEX           149\n",
      "AGE           149\n",
      "             ... \n",
      "FEATURE_51    149\n",
      "FEATURE_52    149\n",
      "FEATURE_53    149\n",
      "FEATURE_54    149\n",
      "FEATURE_55    149\n",
      "Length: 65, dtype: int64\n",
      "9.0\n",
      "ID_1          203\n",
      "ID_2          203\n",
      "group_id      203\n",
      "SEX           203\n",
      "AGE           203\n",
      "             ... \n",
      "FEATURE_51    203\n",
      "FEATURE_52    203\n",
      "FEATURE_53    203\n",
      "FEATURE_54    203\n",
      "FEATURE_55    203\n",
      "Length: 65, dtype: int64\n",
      "---Number of Non-Depression and Depression---\n",
      "13346 1802\n",
      "torch.Size([10906, 56]) torch.Size([10906]) torch.Size([2727, 56]) torch.Size([2727]) torch.Size([1515, 56]) torch.Size([1515])\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('..\\data\\Threshold_3_Operator_-_Depressionfeature_BP_PHQ_9_PercentofDataset_100.csv')\n",
    "print_information(df)\n",
    "# 0.9 train, 0.1 test\n",
    "train_df, test_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "# 0.8 train, 0.2 validation\n",
    "train_df, validation_df = train_test_split(train_df, train_size=0.8, random_state=42)\n",
    "\n",
    "train_features, train_targets = df_to_tensor(train_df,features_column='FEATURES', target_col='Depression')\n",
    "validation_features, validation_targets = df_to_tensor(validation_df, features_column='FEATURES', target_col='Depression')\n",
    "test_features, test_targets = df_to_tensor(test_df, features_column='FEATURES', target_col='Depression')\n",
    "\n",
    "print(train_features.shape, train_targets.shape, validation_features.shape, validation_targets.shape, test_features.shape, test_targets.shape)\n",
    "\n",
    "train_dataset = CustomDataset(train_features, train_targets)\n",
    "validation_dataset = CustomDataset(validation_features, validation_targets)\n",
    "test_dataset = CustomDataset(test_features, test_targets)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=32, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Training Loss: 0.3588, Validation Loss: 0.3739\n",
      "Epoch [2/30], Training Loss: 0.3335, Validation Loss: 0.3770\n",
      "Epoch [3/30], Training Loss: 0.3228, Validation Loss: 0.3533\n",
      "Epoch [4/30], Training Loss: 0.3134, Validation Loss: 0.3531\n",
      "Epoch [5/30], Training Loss: 0.3082, Validation Loss: 0.3504\n",
      "Epoch [6/30], Training Loss: 0.3011, Validation Loss: 0.3304\n",
      "Epoch [7/30], Training Loss: 0.2955, Validation Loss: 0.3342\n",
      "Epoch [8/30], Training Loss: 0.2891, Validation Loss: 0.3237\n",
      "Epoch [9/30], Training Loss: 0.2844, Validation Loss: 0.3203\n",
      "Epoch [10/30], Training Loss: 0.2796, Validation Loss: 0.3125\n",
      "Epoch [11/30], Training Loss: 0.2745, Validation Loss: 0.3252\n",
      "Epoch [12/30], Training Loss: 0.2720, Validation Loss: 0.3060\n",
      "Epoch [13/30], Training Loss: 0.2697, Validation Loss: 0.3059\n",
      "Epoch [14/30], Training Loss: 0.2645, Validation Loss: 0.3177\n",
      "Epoch [15/30], Training Loss: 0.2649, Validation Loss: 0.3022\n",
      "Epoch [16/30], Training Loss: 0.2602, Validation Loss: 0.3010\n",
      "Epoch [17/30], Training Loss: 0.2591, Validation Loss: 0.3053\n",
      "Epoch [18/30], Training Loss: 0.2565, Validation Loss: 0.3080\n",
      "Epoch [19/30], Training Loss: 0.2549, Validation Loss: 0.3134\n",
      "Epoch [20/30], Training Loss: 0.2530, Validation Loss: 0.3004\n",
      "Epoch [21/30], Training Loss: 0.2510, Validation Loss: 0.2955\n",
      "Epoch [22/30], Training Loss: 0.2493, Validation Loss: 0.2932\n",
      "Epoch [23/30], Training Loss: 0.2485, Validation Loss: 0.2907\n",
      "Epoch [24/30], Training Loss: 0.2466, Validation Loss: 0.2944\n",
      "Epoch [25/30], Training Loss: 0.2458, Validation Loss: 0.3163\n",
      "Epoch [26/30], Training Loss: 0.2449, Validation Loss: 0.3090\n",
      "Epoch [27/30], Training Loss: 0.2431, Validation Loss: 0.2959\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 30\u001b[0m\n\u001b[0;32m     26\u001b[0m     m \u001b[38;5;241m=\u001b[39m Depression_Classifier_v_2(input_size, hidden_size)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     28\u001b[0m o \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(m\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[1;32m---> 30\u001b[0m \u001b[43mtrain_model_cross_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvalidation_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m validation_loss, accuracy \u001b[38;5;241m=\u001b[39m evaluate_model(m, test_dataloader, criterion, device)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Store results\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pier1\\OneDrive\\Desktop\\uni\\Master\\2.Semester\\Machine Learning (WIWI)\\Project\\Data for depression\\Pier\\NN.py:143\u001b[0m, in \u001b[0;36mtrain_model_cross_validation\u001b[1;34m(model, train_dataloader, validation_dataloader, criterion, optimizer, num_epochs, device)\u001b[0m\n\u001b[0;32m    141\u001b[0m patience \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m  \u001b[38;5;66;03m# Number of epochs to wait if no improvement is observed\u001b[39;00m\n\u001b[0;32m    142\u001b[0m min_delta \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m  \u001b[38;5;66;03m# Minimum change in the monitored quantity to qualify as improvement\u001b[39;00m\n\u001b[1;32m--> 143\u001b[0m best_val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    144\u001b[0m current_patience \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    146\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[1;32mc:\\Users\\pier1\\anaconda3\\envs\\ML\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pier1\\anaconda3\\envs\\ML\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pier1\\OneDrive\\Desktop\\uni\\Master\\2.Semester\\Machine Learning (WIWI)\\Project\\Data for depression\\Pier\\NN.py:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m     51\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures[idx], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtargets[idx]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mDepression_Classifier_v_0\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_size, hidden_size):\n\u001b[0;32m     56\u001b[0m         \u001b[38;5;28msuper\u001b[39m(Depression_Classifier_v_0, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\pier1\\anaconda3\\envs\\ML\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pier1\\anaconda3\\envs\\ML\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pier1\\anaconda3\\envs\\ML\\lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "input_size = 56\n",
    "hidden_size = 128\n",
    "num_epochs = 30\n",
    "learning_rate_values = [0.001,0.003,0.006,0.01]\n",
    "momentum_values = [0.9, 0.95,0.99]\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Model is bad at 94 % accuracy, because 94 percent of the data is not depressed (PHQ_9)\n",
    "# Initialize the model, criterion, and optimizer\n",
    "#model = Depression_Classifier_v_0(input_size, hidden_size).to(device)\n",
    "results = {}\n",
    "#criterion = nn.BCEWithLogitsLoss()  # Combines sigmoid and binary cross-entropy loss\n",
    "criterion = nn.BCELoss()\n",
    "optimizers = ['Adam', 'SGD']\n",
    "\n",
    "for i in range(3):\n",
    "    for optimizer in optimizers:\n",
    "        if optimizer == 'Adam':\n",
    "            for learning_rate in learning_rate_values:\n",
    "                if i == 0:\n",
    "                    m = Depression_Classifier_v_0(input_size, hidden_size).to(device)\n",
    "                elif i == 1:\n",
    "                    m = Depression_Classifier_v_1(input_size, hidden_size).to(device)\n",
    "                else:\n",
    "                    m = Depression_Classifier_v_2(input_size, hidden_size).to(device)\n",
    "\n",
    "                o = optim.Adam(m.parameters(), lr=learning_rate)\n",
    "\n",
    "                train_model_cross_validation(m, train_dataloader,validation_dataloader,criterion, o, num_epochs, device)\n",
    "\n",
    "                validation_loss, accuracy = evaluate_model(m, test_dataloader, criterion, device)\n",
    "\n",
    "                # Store results\n",
    "                results[(f'Model_{i}', optimizer, learning_rate)] = {\n",
    "                    'validation_loss': validation_loss,\n",
    "                    'accuracy': accuracy\n",
    "                }\n",
    "        else:\n",
    "            for momentum, learning_rate in itertools.product(momentum_values, learning_rate_values):\n",
    "                if i == 0:\n",
    "                    m = Depression_Classifier_v_0(input_size, hidden_size).to(device)\n",
    "                elif i == 1:\n",
    "                    m = Depression_Classifier_v_1(input_size, hidden_size).to(device)\n",
    "                else:\n",
    "                    m = Depression_Classifier_v_2(input_size, hidden_size).to(device)\n",
    "                    \n",
    "                o = optim.SGD(m.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "                train_model_cross_validation(m, train_dataloader,validation_dataloader,criterion, o, num_epochs, device)\n",
    "\n",
    "                validation_loss, accuracy = evaluate_model(m, test_dataloader, criterion, device)\n",
    "\n",
    "                # Store results\n",
    "                results[(f'Model_{i}', optimizer, learning_rate,momentum)] = {\n",
    "                    'validation_loss': validation_loss,\n",
    "                    'accuracy': accuracy\n",
    "                }\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('Model_0', 'Adam', 0.001): {'validation_loss': 0.42725964880629674, 'accuracy': 82.09143457698372}, ('Model_0', 'Adam', 0.002): {'validation_loss': 0.4523360274682109, 'accuracy': 80.45191802417236}, ('Model_0', 'Adam', 0.005): {'validation_loss': 0.48782783526702217, 'accuracy': 78.66526537046768}, ('Model_0', 'Adam', 0.01): {'validation_loss': 0.5232279593712531, 'accuracy': 76.04834471886495}, ('Model_0', 'SGD', 0.001, 0.9): {'validation_loss': 0.4867494810347589, 'accuracy': 78.5496584340515}, ('Model_0', 'SGD', 0.002, 0.9): {'validation_loss': 0.4868668759829246, 'accuracy': 78.2764056752496}, ('Model_0', 'SGD', 0.005, 0.9): {'validation_loss': 0.451741439384102, 'accuracy': 80.69364161849711}, ('Model_0', 'SGD', 0.01, 0.9): {'validation_loss': 0.44255475809910155, 'accuracy': 80.90383604834471}, ('Model_0', 'SGD', 0.001, 0.95): {'validation_loss': 0.5041491652795133, 'accuracy': 78.2238570677877}, ('Model_0', 'SGD', 0.002, 0.95): {'validation_loss': 0.45157863334721365, 'accuracy': 80.30478192327904}, ('Model_0', 'SGD', 0.005, 0.95): {'validation_loss': 0.4542343197452942, 'accuracy': 80.4308985811876}, ('Model_0', 'SGD', 0.01, 0.95): {'validation_loss': 0.4615557696795304, 'accuracy': 79.34839726747241}, ('Model_0', 'SGD', 0.001, 0.98): {'validation_loss': 0.4498110586564813, 'accuracy': 81.02995270625328}, ('Model_0', 'SGD', 0.002, 0.98): {'validation_loss': 0.4618628823617161, 'accuracy': 79.85286389910668}, ('Model_0', 'SGD', 0.005, 0.98): {'validation_loss': 0.4865366102465047, 'accuracy': 78.89647924330005}, ('Model_0', 'SGD', 0.01, 0.98): {'validation_loss': 0.5339582993480183, 'accuracy': 76.80504466631635}, ('Model_1', 'Adam', 0.001): {'validation_loss': 0.30191119362743907, 'accuracy': 88.34471886495008}, ('Model_1', 'Adam', 0.002): {'validation_loss': 0.3412325533744473, 'accuracy': 86.3478717813978}, ('Model_1', 'Adam', 0.005): {'validation_loss': 0.4812794264730991, 'accuracy': 78.46558066211246}, ('Model_1', 'Adam', 0.01): {'validation_loss': 0.5027189858807813, 'accuracy': 77.50919600630583}, ('Model_1', 'SGD', 0.001, 0.9): {'validation_loss': 0.4489987157035194, 'accuracy': 80.56752496058854}, ('Model_1', 'SGD', 0.002, 0.9): {'validation_loss': 0.39157603651085154, 'accuracy': 83.97267472411981}, ('Model_1', 'SGD', 0.005, 0.9): {'validation_loss': 0.3583594456615064, 'accuracy': 84.93956910141881}, ('Model_1', 'SGD', 0.01, 0.9): {'validation_loss': 0.3127739235458758, 'accuracy': 87.32527588018918}, ('Model_1', 'SGD', 0.001, 0.95): {'validation_loss': 0.39096104808701765, 'accuracy': 83.97267472411981}, ('Model_1', 'SGD', 0.002, 0.95): {'validation_loss': 0.36632976698495395, 'accuracy': 84.86600105097214}, ('Model_1', 'SGD', 0.005, 0.95): {'validation_loss': 0.3538595202485187, 'accuracy': 85.73830793483972}, ('Model_1', 'SGD', 0.01, 0.95): {'validation_loss': 0.33444300641449504, 'accuracy': 86.90488702049396}, ('Model_1', 'SGD', 0.001, 0.98): {'validation_loss': 0.3605173669205416, 'accuracy': 85.35995796111403}, ('Model_1', 'SGD', 0.002, 0.98): {'validation_loss': 0.3597855645688188, 'accuracy': 85.61219127693116}, ('Model_1', 'SGD', 0.005, 0.98): {'validation_loss': 0.37128267127195463, 'accuracy': 84.80294272201786}, ('Model_1', 'SGD', 0.01, 0.98): {'validation_loss': 0.4561371924293121, 'accuracy': 80.44140830267997}, ('Model_2', 'Adam', 0.001): {'validation_loss': 0.26970718641009106, 'accuracy': 89.60588544403574}, ('Model_2', 'Adam', 0.002): {'validation_loss': 0.2741003333772189, 'accuracy': 89.12243825538623}, ('Model_2', 'Adam', 0.005): {'validation_loss': 0.5410893101420179, 'accuracy': 76.37414608512874}, ('Model_2', 'Adam', 0.01): {'validation_loss': 0.5654675485503754, 'accuracy': 74.87125591171834}, ('Model_2', 'SGD', 0.001, 0.9): {'validation_loss': 0.49453571628804177, 'accuracy': 78.3289542827115}, ('Model_2', 'SGD', 0.002, 0.9): {'validation_loss': 0.47362697954545885, 'accuracy': 79.2958486600105}, ('Model_2', 'SGD', 0.005, 0.9): {'validation_loss': 0.3957542545043382, 'accuracy': 83.45769837099317}, ('Model_2', 'SGD', 0.01, 0.9): {'validation_loss': 0.3295859342453464, 'accuracy': 86.7157120336311}, ('Model_2', 'SGD', 0.001, 0.95): {'validation_loss': 0.4744786216108591, 'accuracy': 79.28533893851812}, ('Model_2', 'SGD', 0.002, 0.95): {'validation_loss': 0.4141246758451398, 'accuracy': 82.51182343667892}, ('Model_2', 'SGD', 0.005, 0.95): {'validation_loss': 0.33644015624519163, 'accuracy': 86.09563846558066}, ('Model_2', 'SGD', 0.01, 0.95): {'validation_loss': 0.33393422871728073, 'accuracy': 86.42143983184445}, ('Model_2', 'SGD', 0.001, 0.98): {'validation_loss': 0.40170821897535514, 'accuracy': 83.36311087756174}, ('Model_2', 'SGD', 0.002, 0.98): {'validation_loss': 0.3523095207626388, 'accuracy': 85.20231213872832}, ('Model_2', 'SGD', 0.005, 0.98): {'validation_loss': 0.33488112195526193, 'accuracy': 86.72622175512349}, ('Model_2', 'SGD', 0.01, 0.98): {'validation_loss': 0.4318991479957664, 'accuracy': 81.37677351550184}}\n",
      "{('Model_0', 'Adam', 0.001, None): {'validation_loss': 0.42725964880629674, 'accuracy': 82.09143457698372}, ('Model_0', 'Adam', 0.002, None): {'validation_loss': 0.4523360274682109, 'accuracy': 80.45191802417236}, ('Model_0', 'Adam', 0.005, None): {'validation_loss': 0.48782783526702217, 'accuracy': 78.66526537046768}, ('Model_0', 'Adam', 0.01, None): {'validation_loss': 0.5232279593712531, 'accuracy': 76.04834471886495}, ('Model_0', 'SGD', 0.001, 0.9): {'validation_loss': 0.4867494810347589, 'accuracy': 78.5496584340515}, ('Model_0', 'SGD', 0.002, 0.9): {'validation_loss': 0.4868668759829246, 'accuracy': 78.2764056752496}, ('Model_0', 'SGD', 0.005, 0.9): {'validation_loss': 0.451741439384102, 'accuracy': 80.69364161849711}, ('Model_0', 'SGD', 0.01, 0.9): {'validation_loss': 0.44255475809910155, 'accuracy': 80.90383604834471}, ('Model_0', 'SGD', 0.001, 0.95): {'validation_loss': 0.5041491652795133, 'accuracy': 78.2238570677877}, ('Model_0', 'SGD', 0.002, 0.95): {'validation_loss': 0.45157863334721365, 'accuracy': 80.30478192327904}, ('Model_0', 'SGD', 0.005, 0.95): {'validation_loss': 0.4542343197452942, 'accuracy': 80.4308985811876}, ('Model_0', 'SGD', 0.01, 0.95): {'validation_loss': 0.4615557696795304, 'accuracy': 79.34839726747241}, ('Model_0', 'SGD', 0.001, 0.98): {'validation_loss': 0.4498110586564813, 'accuracy': 81.02995270625328}, ('Model_0', 'SGD', 0.002, 0.98): {'validation_loss': 0.4618628823617161, 'accuracy': 79.85286389910668}, ('Model_0', 'SGD', 0.005, 0.98): {'validation_loss': 0.4865366102465047, 'accuracy': 78.89647924330005}, ('Model_0', 'SGD', 0.01, 0.98): {'validation_loss': 0.5339582993480183, 'accuracy': 76.80504466631635}, ('Model_1', 'Adam', 0.001, None): {'validation_loss': 0.30191119362743907, 'accuracy': 88.34471886495008}, ('Model_1', 'Adam', 0.002, None): {'validation_loss': 0.3412325533744473, 'accuracy': 86.3478717813978}, ('Model_1', 'Adam', 0.005, None): {'validation_loss': 0.4812794264730991, 'accuracy': 78.46558066211246}, ('Model_1', 'Adam', 0.01, None): {'validation_loss': 0.5027189858807813, 'accuracy': 77.50919600630583}, ('Model_1', 'SGD', 0.001, 0.9): {'validation_loss': 0.4489987157035194, 'accuracy': 80.56752496058854}, ('Model_1', 'SGD', 0.002, 0.9): {'validation_loss': 0.39157603651085154, 'accuracy': 83.97267472411981}, ('Model_1', 'SGD', 0.005, 0.9): {'validation_loss': 0.3583594456615064, 'accuracy': 84.93956910141881}, ('Model_1', 'SGD', 0.01, 0.9): {'validation_loss': 0.3127739235458758, 'accuracy': 87.32527588018918}, ('Model_1', 'SGD', 0.001, 0.95): {'validation_loss': 0.39096104808701765, 'accuracy': 83.97267472411981}, ('Model_1', 'SGD', 0.002, 0.95): {'validation_loss': 0.36632976698495395, 'accuracy': 84.86600105097214}, ('Model_1', 'SGD', 0.005, 0.95): {'validation_loss': 0.3538595202485187, 'accuracy': 85.73830793483972}, ('Model_1', 'SGD', 0.01, 0.95): {'validation_loss': 0.33444300641449504, 'accuracy': 86.90488702049396}, ('Model_1', 'SGD', 0.001, 0.98): {'validation_loss': 0.3605173669205416, 'accuracy': 85.35995796111403}, ('Model_1', 'SGD', 0.002, 0.98): {'validation_loss': 0.3597855645688188, 'accuracy': 85.61219127693116}, ('Model_1', 'SGD', 0.005, 0.98): {'validation_loss': 0.37128267127195463, 'accuracy': 84.80294272201786}, ('Model_1', 'SGD', 0.01, 0.98): {'validation_loss': 0.4561371924293121, 'accuracy': 80.44140830267997}, ('Model_2', 'Adam', 0.001, None): {'validation_loss': 0.26970718641009106, 'accuracy': 89.60588544403574}, ('Model_2', 'Adam', 0.002, None): {'validation_loss': 0.2741003333772189, 'accuracy': 89.12243825538623}, ('Model_2', 'Adam', 0.005, None): {'validation_loss': 0.5410893101420179, 'accuracy': 76.37414608512874}, ('Model_2', 'Adam', 0.01, None): {'validation_loss': 0.5654675485503754, 'accuracy': 74.87125591171834}, ('Model_2', 'SGD', 0.001, 0.9): {'validation_loss': 0.49453571628804177, 'accuracy': 78.3289542827115}, ('Model_2', 'SGD', 0.002, 0.9): {'validation_loss': 0.47362697954545885, 'accuracy': 79.2958486600105}, ('Model_2', 'SGD', 0.005, 0.9): {'validation_loss': 0.3957542545043382, 'accuracy': 83.45769837099317}, ('Model_2', 'SGD', 0.01, 0.9): {'validation_loss': 0.3295859342453464, 'accuracy': 86.7157120336311}, ('Model_2', 'SGD', 0.001, 0.95): {'validation_loss': 0.4744786216108591, 'accuracy': 79.28533893851812}, ('Model_2', 'SGD', 0.002, 0.95): {'validation_loss': 0.4141246758451398, 'accuracy': 82.51182343667892}, ('Model_2', 'SGD', 0.005, 0.95): {'validation_loss': 0.33644015624519163, 'accuracy': 86.09563846558066}, ('Model_2', 'SGD', 0.01, 0.95): {'validation_loss': 0.33393422871728073, 'accuracy': 86.42143983184445}, ('Model_2', 'SGD', 0.001, 0.98): {'validation_loss': 0.40170821897535514, 'accuracy': 83.36311087756174}, ('Model_2', 'SGD', 0.002, 0.98): {'validation_loss': 0.3523095207626388, 'accuracy': 85.20231213872832}, ('Model_2', 'SGD', 0.005, 0.98): {'validation_loss': 0.33488112195526193, 'accuracy': 86.72622175512349}, ('Model_2', 'SGD', 0.01, 0.98): {'validation_loss': 0.4318991479957664, 'accuracy': 81.37677351550184}}\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "models_df = []\n",
    "optimizer_df = []\n",
    "learning_rate_df = []\n",
    "momentum_df = []\n",
    "accuracy_df = []\n",
    "validation_loss_df = []\n",
    "for key,value in results.items():\n",
    "    accuracy_df.append(value['accuracy'])\n",
    "    validation_loss_df.append(value['validation_loss'])\n",
    "    opti = key[1]\n",
    "    models_df.append(key[0])\n",
    "    optimizer_df.append(opti)\n",
    "    learning_rate_df.append(key[2])\n",
    "    if opti == 'SGD':\n",
    "        momentum_df.append(key[3])\n",
    "    else:\n",
    "        momentum_df.append(None)\n",
    "        \n",
    "df['Model'] = models_df\n",
    "df['Optimizer'] = optimizer_df\n",
    "df['Learning Rate'] = learning_rate_df\n",
    "df['Momentum'] = momentum_df\n",
    "df['Validation Loss'] = validation_loss_df\n",
    "df['Accuracy'] = accuracy_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Model Optimizer  Learning Rate  Momentum  Validation Loss   Accuracy\n",
      "0   Model_0      Adam          0.001       NaN         0.427260  82.091435\n",
      "1   Model_0      Adam          0.002       NaN         0.452336  80.451918\n",
      "2   Model_0      Adam          0.005       NaN         0.487828  78.665265\n",
      "3   Model_0      Adam          0.010       NaN         0.523228  76.048345\n",
      "4   Model_0       SGD          0.001      0.90         0.486749  78.549658\n",
      "5   Model_0       SGD          0.002      0.90         0.486867  78.276406\n",
      "6   Model_0       SGD          0.005      0.90         0.451741  80.693642\n",
      "7   Model_0       SGD          0.010      0.90         0.442555  80.903836\n",
      "8   Model_0       SGD          0.001      0.95         0.504149  78.223857\n",
      "9   Model_0       SGD          0.002      0.95         0.451579  80.304782\n",
      "10  Model_0       SGD          0.005      0.95         0.454234  80.430899\n",
      "11  Model_0       SGD          0.010      0.95         0.461556  79.348397\n",
      "12  Model_0       SGD          0.001      0.98         0.449811  81.029953\n",
      "13  Model_0       SGD          0.002      0.98         0.461863  79.852864\n",
      "14  Model_0       SGD          0.005      0.98         0.486537  78.896479\n",
      "15  Model_0       SGD          0.010      0.98         0.533958  76.805045\n",
      "16  Model_1      Adam          0.001       NaN         0.301911  88.344719\n",
      "17  Model_1      Adam          0.002       NaN         0.341233  86.347872\n",
      "18  Model_1      Adam          0.005       NaN         0.481279  78.465581\n",
      "19  Model_1      Adam          0.010       NaN         0.502719  77.509196\n",
      "20  Model_1       SGD          0.001      0.90         0.448999  80.567525\n",
      "21  Model_1       SGD          0.002      0.90         0.391576  83.972675\n",
      "22  Model_1       SGD          0.005      0.90         0.358359  84.939569\n",
      "23  Model_1       SGD          0.010      0.90         0.312774  87.325276\n",
      "24  Model_1       SGD          0.001      0.95         0.390961  83.972675\n",
      "25  Model_1       SGD          0.002      0.95         0.366330  84.866001\n",
      "26  Model_1       SGD          0.005      0.95         0.353860  85.738308\n",
      "27  Model_1       SGD          0.010      0.95         0.334443  86.904887\n",
      "28  Model_1       SGD          0.001      0.98         0.360517  85.359958\n",
      "29  Model_1       SGD          0.002      0.98         0.359786  85.612191\n",
      "30  Model_1       SGD          0.005      0.98         0.371283  84.802943\n",
      "31  Model_1       SGD          0.010      0.98         0.456137  80.441408\n",
      "32  Model_2      Adam          0.001       NaN         0.269707  89.605885\n",
      "33  Model_2      Adam          0.002       NaN         0.274100  89.122438\n",
      "34  Model_2      Adam          0.005       NaN         0.541089  76.374146\n",
      "35  Model_2      Adam          0.010       NaN         0.565468  74.871256\n",
      "36  Model_2       SGD          0.001      0.90         0.494536  78.328954\n",
      "37  Model_2       SGD          0.002      0.90         0.473627  79.295849\n",
      "38  Model_2       SGD          0.005      0.90         0.395754  83.457698\n",
      "39  Model_2       SGD          0.010      0.90         0.329586  86.715712\n",
      "40  Model_2       SGD          0.001      0.95         0.474479  79.285339\n",
      "41  Model_2       SGD          0.002      0.95         0.414125  82.511823\n",
      "42  Model_2       SGD          0.005      0.95         0.336440  86.095638\n",
      "43  Model_2       SGD          0.010      0.95         0.333934  86.421440\n",
      "44  Model_2       SGD          0.001      0.98         0.401708  83.363111\n",
      "45  Model_2       SGD          0.002      0.98         0.352310  85.202312\n",
      "46  Model_2       SGD          0.005      0.98         0.334881  86.726222\n",
      "47  Model_2       SGD          0.010      0.98         0.431899  81.376774\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('FNN_BP_PHQ_9_minus.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the csv data to compare the different models and cross-validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the models to predict the test data and use the confusion matrix and their metrices (accurary, precision, recall and f1-score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
