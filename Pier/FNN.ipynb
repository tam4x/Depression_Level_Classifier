{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the path to the directory where your module is located\n",
    "module_path = os.path.abspath(os.path.join('..', 'C:\\\\Users\\\\pier1\\\\OneDrive\\\\Desktop\\\\uni\\\\Master\\\\2.Semester\\\\Machine Learning (WIWI)\\\\Project\\\\Data for depression\\\\'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Now you can import your module\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from NN import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('..\\data\\Threshold_3_Operator_-_Depressionfeature_BP_PHQ_9_PercentofDataset_100.csv')\n",
    "print_information(df)\n",
    "# 0.9 train, 0.1 test\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "# 0.8 train, 0.2 validation\n",
    "#train_df, validation_df = train_test_split(train_df, train_size=0.8, random_state=42)\n",
    "\n",
    "train_features, train_targets = df_to_tensor(train_df,features_column='FEATURES', target_col='Depression')\n",
    "#validation_features, validation_targets = df_to_tensor(validation_df, features_column='FEATURES', target_col='Depression')\n",
    "test_features, test_targets = df_to_tensor(test_df, features_column='FEATURES', target_col='Depression')\n",
    "\n",
    "#print(train_features.shape, train_targets.shape, validation_features.shape, validation_targets.shape, test_features.shape, test_targets.shape)\n",
    "print(train_features.shape, train_targets.shape, test_features.shape, test_targets.shape)\n",
    "\n",
    "train_dataset = CustomDataset(train_features, train_targets)\n",
    "#validation_dataset = CustomDataset(validation_features, validation_targets)\n",
    "test_dataset = CustomDataset(test_features, test_targets)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "#validation_dataloader = DataLoader(validation_dataset, batch_size=32, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "input_size = 56\n",
    "hidden_size = 128\n",
    "num_epochs = 60\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Model is bad at 94 % accuracy, because 94 percent of the data is not depressed (PHQ_9)\n",
    "# Initialize the model, criterion, and optimizer\n",
    "model = Depression_Classifier_v_2(input_size, hidden_size).to(device)\n",
    "#criterion = nn.BCEWithLogitsLoss()  # Combines sigmoid and binary cross-entropy loss\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "optimizers = ['Adam', 'SGD']\n",
    "train_model(model, train_dataloader, criterion, optimizer, num_epochs, device)\n",
    "evaluate_model(model, test_dataloader, criterion, device)\n",
    "\n",
    "#results = {}\n",
    "#results = cross_validation(train_dataloader, validation_dataloader, input_size, hidden_size, criterion, optimizers, device, results)\n",
    "\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "models_df = []\n",
    "optimizer_df = []\n",
    "learning_rate_df = []\n",
    "momentum_df = []\n",
    "accuracy_df = []\n",
    "validation_loss_df = []\n",
    "for key,value in results.items():\n",
    "    accuracy_df.append(value['accuracy'])\n",
    "    validation_loss_df.append(value['validation_loss'])\n",
    "    opti = key[1]\n",
    "    models_df.append(key[0])\n",
    "    optimizer_df.append(opti)\n",
    "    learning_rate_df.append(key[2])\n",
    "    if opti == 'SGD':\n",
    "        momentum_df.append(key[3])\n",
    "    else:\n",
    "        momentum_df.append(None)\n",
    "        \n",
    "df['Model'] = models_df\n",
    "df['Optimizer'] = optimizer_df\n",
    "df['Learning Rate'] = learning_rate_df\n",
    "df['Momentum'] = momentum_df\n",
    "df['Validation Loss'] = validation_loss_df\n",
    "df['Accuracy'] = accuracy_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('FNN_MH_PHQ_S_minus.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the csv data to compare the different models and cross-validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_phq_9 = pd.read_csv('FNN_BP_PHQ_9_minus.csv')\n",
    "mh_phq_s = pd.read_csv('FNN_MH_PHQ_S_minus.csv')\n",
    "\n",
    "max_index_phq9 = bp_phq_9['Accuracy'].idxmax()\n",
    "max_index_mh_phq_s = mh_phq_s['Accuracy'].idxmax()\n",
    "\n",
    "print(bp_phq_9.loc[max_index_phq9])\n",
    "print(mh_phq_s.loc[max_index_mh_phq_s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Based on the cross-validation with different learning rates, momentum values, optimizers, models and 30 epochs the best accuracy is based of\n",
    "- Model_2, Adam, lr = 0.001 with a accuracy of 94% with the depression_classifier PHQ_9\n",
    "- Model_2, Adam, lr = 0.001 with a accuracy of 83% with the depression_classifier MH_PHQ_s\n",
    "- Using these values we run for 60 epochs\n",
    "- Model_2, Adam, lr = 0.001 with a accuracy of 89% with the depression_classifier MH_PHQ_s\n",
    "- Model_2, Adam, lr = 0.001 with a accuracy of 95% with the depression_classifier PHQ_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'BP_PHQ_9.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
