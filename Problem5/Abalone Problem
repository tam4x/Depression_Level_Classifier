https://kunjmehta10.medium.com/intro-to-machine-learning-via-the-abalone-age-prediction-problem-4e290a8b2ed3


# -*- coding: utf-8 -*-
"""Aufgabe 5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nHTUUm5etoz2e3ErXH8JYEzWTnxLH2tU
"""

pip install ucimlrepo

from ucimlrepo import fetch_ucirepo

# fetch dataset
abalone = fetch_ucirepo(id=1)

# data (as pandas dataframes)
X = abalone.data.features
y = abalone.data.targets

# metadata
# print(abalone.metadata)

# variable information
print(abalone.variables)

X

y

# Commented out IPython magic to ensure Python compatibility.
# Data handling and processing
import pandas as pd
import numpy as np
from pandas import DataFrame
from numpy import loadtxt

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

# Machine Learning models
from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet, LogisticRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
import xgboost as xgb

# Model selection and evaluation
from sklearn.model_selection import train_test_split, KFold, cross_val_predict, GridSearchCV, RandomizedSearchCV
from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, roc_auc_score

# Data fetching
from ucimlrepo import fetch_ucirepo

# Serialization
import joblib

target_url = ("http://archive.ics.uci.edu/ml/machine-"
              "learning-databases/abalone/abalone.data")

abalone = pd.read_csv(target_url,header=None) # Remove the prefix argument
abalone.columns = ['Sex', 'Length', 'Diameter', 'Height',
                   'Whole weight', 'Shucked weight',
                   'Viscera weight', 'Shell weight', 'Rings'] # Set column names explicitly
abalone.head()

plt.figure(figsize=(10, 6))
sns.histplot(abalone['Rings'], kde=True, bins=30)
plt.title('Distribution of Rings')
plt.xlabel('Number of Rings')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

print(y.shape)
print(X.shape)

df = pd.DataFrame(X)

# Assuming 'abalone' is your original DataFrame containing 'Rings'
X = abalone.drop('Rings', axis=1)  # Features
y = abalone['Rings']  # Target variable
df = pd.DataFrame(X)
print(y)

abalone.isnull().sum(axis = 0) # if there are null values
abalone = pd.get_dummies(abalone)
abalone.head()

#Corrleation Map
corMat = DataFrame(abalone.iloc[:,:8].corr()).values
corMat = np.around(corMat, decimals = 3)

feature_importance = DataFrame(abalone.iloc[:,:8].corr()).iloc[:-1, -1].sort_values(ascending=False)
print('Features in Descending Order of Importance', list(feature_importance.index))

columns = ['Length', 'Diameter', 'Height', 'Whole weight', 'Shucked weight', 'Viscera weight', 'Shell weight', 'Rings']
fig, ax = plot.subplots(figsize = (20,60))
im = ax.imshow(corMat)

ax.set_xticks(range(len(corMat)))
ax.set_yticks(range(len(corMat)))
ax.set_xticklabels(columns)
ax.set_yticklabels(columns)

for i in range(8):
    for j in range(8):
        text = ax.text(j, i, corMat[i, j],
                       ha="center", va="center", color="black", size = "20")

n, bins, patches = plot.hist(abalone['Rings'], bins = 29)
y = abalone["Rings"]
X = abalone.drop(columns = "Rings")

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=20)

# Standardize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

#Testing model for best MSE
models = {
    "Linear Regression": LinearRegression(),
    "LASSO Regression": Lasso(),
    "Ridge Regression": Ridge(),
    "Elastic Net": ElasticNet(),
    "Decision Tree": DecisionTreeRegressor(),
    "Random Forest": RandomForestRegressor(),
    "Gradient Boosting": GradientBoostingRegressor(),
    "XGBoost": xgb.XGBRegressor(),
    "AdaBoost": AdaBoostRegressor(),
    "Support Vector Machine": SVR(),
    "K-Nearest Neighbors": KNeighborsRegressor(),
    "Logistic Regression": LogisticRegression(max_iter=1000)
}

cv = KFold(n_splits=5, random_state=20, shuffle=True)

# Evaluate models with both MSE and R^2 metrics
results = {}
for name, model in models.items():
    y_pred = cross_val_predict(model, X_train, y_train, cv=cv)
    mse = mean_squared_error(y_train, y_pred)
    r2 = r2_score(y_train, y_pred)
    results[name] = {'MSE': mse, 'R^2': r2}

df_results = pd.DataFrame.from_dict(results, orient='index')
df_results_sorted = df_results.sort_values('MSE')
print(df_results_sorted)

# Hyperparameter tuning for the best parameters
param_grids = {
    "Gradient Boosting": {
        'n_estimators': [100, 300],
        'learning_rate': [0.05, 0.1],
        'max_depth': [3, 5],
        'subsample': [0.8, 1.0]
    },
    "Random Forest": {
        'n_estimators': [100, 300],
        'max_depth': [10, 20],
        'min_samples_split': [2, 5],
        'min_samples_leaf': [1, 2]
    },
    "Ridge Regression": {
        'alpha': [0.1, 1.0, 10.0]
    }
}

models = {
    "Gradient Boosting": GradientBoostingRegressor(random_state=42),
    "Random Forest": RandomForestRegressor(random_state=42),
    "Ridge Regression": Ridge()
}

cv = KFold(n_splits=5, random_state=42, shuffle=True)

# Perform GridSearchCV for each model
best_estimators = {}
for model_name, model in models.items():
    print(f"Tuning hyperparameters for {model_name}...")
    grid_search = GridSearchCV(estimator=model, param_grid=param_grids[model_name], cv=cv, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)
    grid_search.fit(X_train_scaled, y_train)
    best_estimators[model_name] = grid_search.best_estimator_
    print(f"Best parameters for {model_name}: {grid_search.best_params_}")
    print(f"Best cross-validated MSE for {model_name}: {grid_search.best_score_}")

test_results = {}
for model_name, model in best_estimators.items():
    y_pred_test = model.predict(X_test_scaled)
    test_mse = mean_squared_error(y_test, y_pred_test)
    test_results[model_name] = test_mse
    print(f"Test MSE for {model_name}: {test_mse}")

print(test_results)

# Train and evaluate models using GridSearchCV
results = []
for model_name, model in models.items():
    print(f"Training {model_name}...")
    grid_search = GridSearchCV(model, param_grids[model_name], cv=5, scoring='neg_mean_squared_error')
    grid_search.fit(X_train_scaled, y_train)
    best_model = grid_search.best_estimator_
    y_pred = best_model.predict(X_test_scaled)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    results.append((model_name, grid_search.best_params_, mse, r2, best_model, y_pred))


# Display the results
for result in results:
    print(f"Model: {result[0]}")
    print(f"Best Parameters: {result[1]}")
    print(f"Test MSE: {result[2]}")
    print(f"Test R^2: {result[3]}")
    print()

# Plot feature importances
if 'Random Forest' in best_estimators:
    rf_model = best_estimators['Random Forest']
    rf_importances = pd.Series(rf_model.feature_importances_, index=X_train.columns)
    rf_importances.nlargest(10).plot(kind='barh')
    plt.title('Top 10 Feature Importances for Random Forest')
    plt.show()

if 'Gradient Boosting' in best_estimators:
    gb_model = best_estimators['Gradient Boosting']
    gb_importances = pd.Series(gb_model.feature_importances_, index=X_train.columns)
    gb_importances.nlargest(10).plot(kind='barh')
    plt.title('Top 10 Feature Importances for Gradient Boosting')
    plt.show()

if 'Ridge Regression' in best_estimators:
    ridge_model = best_estimators['Ridge Regression']
    ridge_coef = pd.Series(ridge_model.coef_, index=X_train.columns)
    ridge_coef.nlargest(10).plot(kind='barh')
    plt.title('Top 10 Feature Importances for Ridge Regression')
    plt.xlabel('Coefficient Value')
    plt.ylabel('Features')
    plt.show()

# Perform cross-validation on all the best models
from sklearn.model_selection import cross_val_score
cv_results = {}
for model_name, model in best_estimators.items():

    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=cv, scoring='neg_mean_squared_error', n_jobs=-1)


    cv_mse_scores = -cv_scores
    mean_cv_mse = cv_mse_scores.mean()
    std_cv_mse = cv_mse_scores.std()

    cv_results[model_name] = {
        'mean_mse': mean_cv_mse,
        'std_mse': std_cv_mse,
        'cv_scores': cv_mse_scores
    }

    print(f"Cross-validated MSE for {model_name}:")
    print(f"Mean MSE: {mean_cv_mse}")
    print(f"Standard Deviation of MSE: {std_cv_mse}")

    # Plot the cross-validation results
    plt.figure(figsize=(10, 6))
    plt.boxplot(cv_mse_scores, vert=False)
    plt.title(f'Cross-Validation MSE for {model_name}')
    plt.xlabel('MSE')
    plt.show()

# Perform cross-validation on the best model
from sklearn.model_selection import cross_val_score

best_model_name = min(test_results, key=test_results.get)
best_model = best_estimators[best_model_name]

cv_scores = cross_val_score(best_model, X_train_scaled, y_train, cv=cv, scoring='neg_mean_squared_error', n_jobs=-1)

cv_mse_scores = -cv_scores
mean_cv_mse = cv_mse_scores.mean()
std_cv_mse = cv_mse_scores.std()

print(f"Cross-validated MSE for the best model ({best_model_name}):")
print(f"Mean MSE: {mean_cv_mse}")
print(f"Standard Deviation of MSE: {std_cv_mse}")

plt.figure(figsize=(10, 6))
plt.boxplot(cv_mse_scores, vert=False)
plt.title(f'Cross-Validation MSE for {best_model_name}')
plt.xlabel('MSE')
plt.show()

import time

X = abalone.drop(columns=['Rings'])
y = abalone['Rings']

# LinearRegression model
model = LinearRegression()

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Leave-One-Out Cross-Validation
loo = LeaveOneOut()
start_time = time.time() # Now time.time() will work

y_true, y_pred = [], []

for train_index, test_index in loo.split(X_scaled):
    X_train, X_test = X_scaled[train_index], X_scaled[test_index]
    y_train, y_test = y[train_index], y[test_index]

    model.fit(X_train, y_train)
    prediction = model.predict(X_test)

    y_true.append(y_test)
    y_pred.append(prediction[0])

# Elapsed time
end_time = time.time()
elapsed_time = end_time - start_time

# Calculate performance metrics
mse = mean_squared_error(y_true, y_pred)
print(f"Mean Squared Error: {mse}")
print(f"Elapsed time for LOOCV with LinearRegression: {elapsed_time:.2f} seconds")

results = pd.DataFrame({'True Values': y_true, 'Predicted Values': y_pred})
results.to_csv('loo_cv_predictions_linear_regression_abalone.csv', index=False)
print("Predictions saved to 'loo_cv_predictions_linear_regression_abalone.csv'")

#Plot
plt.figure(figsize=(10, 6))
hb = plt.hexbin(y_true, y_true, gridsize=30, cmap='cividis', mincnt=1, edgecolors='w', linewidths=0.5)
plt.scatter(y_true, y_pred, alpha=0.5, label='Predicted Values', color='orange')
plt.xlabel('True Values')
plt.ylabel('Predicted Values')
plt.title('True Values vs Predicted Values (LOOCV with Linear Regression)')
plt.legend()
plt.colorbar(hb, label='Density of True Values')
plt.grid(True)
plt.show()

X = abalone.drop(columns=['Rings'])
y = abalone['Rings']

# Ridge model
model = Ridge(alpha=1.0)

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Leave-One-Out Cross-Validation
loo = LeaveOneOut()

# Perform LOOCV
start_time = time.time()

model = Ridge(alpha=1.0)
y_true, y_pred = [], []
for train_index, test_index in loo.split(X_scaled):
    X_train, X_test = X_scaled[train_index], X_scaled[test_index]
    y_train, y_test = y_synthetic[train_index], y_synthetic[test_index]
    model.fit(X_train, y_train)
    prediction = model.predict(X_test)
    y_true.append(y_test)
    y_pred.append(prediction[0])


mse = mean_squared_error(y_true, y_pred)
print(f"Mean Squared Error: {mse}")

results = pd.DataFrame({'True Values': y_true, 'Predicted Values': y_pred})
results.to_csv('loo_cv_predictions_ridge_synthetic.csv', index=False)
print("Predictions saved to 'loo_cv_predictions_ridge_synthetic.csv'")

# Plot
plt.figure(figsize=(10, 6))
hb = plt.hexbin(y_true, y_true, gridsize=30, cmap='cividis', mincnt=1, edgecolors='w', linewidths=0.5)
plt.scatter(y_true, y_pred, alpha=0.5, label='Predicted Values', color='orange')
plt.xlabel('True Values')
plt.ylabel('Predicted Values')
plt.title('True Values vs Predicted Values (LOOCV with Ridge Regression)')
plt.legend()
plt.colorbar(hb, label='Density of True Values')
plt.grid(True)
plt.show()

# Reduce the sample size
abalone_reduced = abalone.sample(n=100, random_state=0)
X = abalone_reduced.drop(columns=['Rings'])
y = abalone_reduced['Rings']

# GradientBoostingRegressor model
model = GradientBoostingRegressor(n_estimators=100, random_state=1)

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Leave-One-Out Cross-Validation
loo = LeaveOneOut()

# Measure the time taken to perform LOOCV
start_time = time.time()

y_true, y_pred = [], []
for train_index, test_index in loo.split(X_scaled):
    X_train, X_test = X_scaled[train_index], X_scaled[test_index]
    y_train, y_test = y_synthetic[train_index], y_synthetic[test_index]
    model.fit(X_train, y_train)
    prediction = model.predict(X_test)
    y_true.append(y_test)
    y_pred.append(prediction[0])


mse = mean_squared_error(y_true, y_pred)
print(f"Mean Squared Error: {mse}")

results = pd.DataFrame({'True Values': y_true, 'Predicted Values': y_pred})
results.to_csv('loo_cv_predictions_gb_synthetic.csv', index=False)
print("Predictions saved to 'loo_cv_predictions_gb_synthetic.csv'")


# Plot
plt.figure(figsize=(10, 6))
hb = plt.hexbin(y_true, y_true, gridsize=30, cmap='cividis', mincnt=1, edgecolors='w', linewidths=0.5)
plt.scatter(y_true, y_pred, alpha=0.5, label='Predicted Values', color='orange')
plt.xlabel('True Values')
plt.ylabel('Predicted Values')
plt.title('True Values vs Predicted Values (LOOCV with Gradient Boosting)')
plt.legend()
plt.colorbar(hb, label='Density of True Values')
plt.grid(True)
plt.show()

abalone_reduced = abalone.sample(n=100, random_state=0)  # Reduce to 100 samples

X = abalone_reduced.drop(columns=['Rings'])
y = abalone_reduced['Rings']

# Reset the index of y to match the reduced DataFrame
y = y.reset_index(drop=True)  # This line is added

# Initialize the RandomForestRegressor model
model = RandomForestRegressor(n_estimators=100, random_state=1)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Leave-One-Out Cross-Validation
loo = LeaveOneOut()

# Measure the time taken to perform LOOCV
start_time = time.time()
y_true, y_pred = [], []
for train_index, test_index in loo.split(X_scaled):
    X_train, X_test = X_scaled[train_index], X_scaled[test_index]
    y_train, y_test = y[train_index], y[test_index]
    model.fit(X_train, y_train)
    prediction = model.predict(X_test)
    y_true.append(y_test)
    y_pred.append(prediction[0])

end_time = time.time()
elapsed_time = end_time - start_time

mse = mean_squared_error(y_true, y_pred)
print(f"Mean Squared Error: {mse}")
print(f"Elapsed time for LOOCV with RandomForestRegressor: {elapsed_time:.2f} seconds")

results = pd.DataFrame({'True Values': y_true, 'Predicted Values': y_pred})
results.to_csv('loo_cv_predictions_rf_reduced.csv', index=False)
print("Predictions saved to 'loo_cv_predictions_rf_reduced.csv'")

# Plot
plt.figure(figsize=(10, 6))
hb = plt.hexbin(y_true, y_true, gridsize=30, cmap='cividis', mincnt=1, edgecolors='w', linewidths=0.5)
plt.scatter(y_true, y_pred, alpha=0.5, label='Predicted Values', color='orange')
plt.xlabel('True Values')
plt.ylabel('Predicted Values')
plt.title('True Values vs Predicted Values (LOOCV with Random Forest)')
plt.legend()
plt.colorbar(hb, label='Density of True Values')
plt.grid(True)
plt.show()
