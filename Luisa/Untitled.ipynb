{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc07a80-216c-4bac-9d57-221fcaf50d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from helpers import *\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "import operator\n",
    "import pywt\n",
    "\n",
    "operator_map = {\n",
    "    '+': operator.add,\n",
    "    '-': operator.sub,\n",
    "    '*': operator.mul,\n",
    "    '/': operator.truediv,\n",
    "    '//': operator.floordiv,\n",
    "    '%': operator.mod,\n",
    "    '**': operator.pow,\n",
    "}\n",
    "\n",
    "class Synthetic_Patient_Dataset:\n",
    "\n",
    "    def __init__(self, threshold : int, actigraphy_data_operator: str, depression_classifier_feature: str, percent_of_dataset: int):\n",
    "        # Goes from -27 to 27 so with absolute from 0 - 27 possible thresholds = [12, 15, 18, 20, 22, 24]\n",
    "        self.threshold = threshold\n",
    "        # Operators include + - * / as a string\n",
    "        self.operator = actigraphy_data_operator\n",
    "        # classifier Feature has to be string and BP_PHQ_9 or MH_PHQ_S BP_PHQ_1 -> BP_PHQ_8 can also be added \n",
    "        self.depression_feature = depression_classifier_feature\n",
    "\n",
    "        self.percent = percent_of_dataset\n",
    "\n",
    "    def load_data(self, path_all, path_pam):\n",
    "        print(f'Loading Datasets from {path_all} and {path_pam}')\n",
    "        self.all14_df=pd.read_sas(path_all + 'hn14_all.sas7bdat')\n",
    "        self.all16_df=pd.read_sas(path_all + 'hn16_all.sas7bdat')\n",
    "        self.pam14_df=pd.read_sas(path_pam + 'HN14_PAM.sas7bdat')\n",
    "        self.pam16_df=pd.read_sas(path_pam + 'hn16_pam.sas7bdat')\n",
    "        \n",
    "    def remove_features(self):\n",
    "        print('Removing Features')\n",
    "        self.all16_df = self.all16_df[[\"ID\", \"year\", \"sex\", \"age\", \"BP_PHQ_9\",\n",
    "                  \"mh_PHQ_S\", \"HE_BMI\", \"mh_stress\", \"EQ5D\"]]\n",
    "        self.all14_df = self.all14_df[[\"id\", \"year\", \"sex\", \"age\", \"BP_PHQ_9\",\n",
    "                        \"mh_PHQ_S\", \"HE_BMI\", \"mh_stress\", \"EQ5D\"]]\n",
    "        \n",
    "        self.all14_df, self.all16_df = process_data(self.all14_df), process_data(self.all16_df)\n",
    "    \n",
    "    def create_intervalls(self):\n",
    "        print('Creating Intervalls')\n",
    "        self.all14_df['HE_BMI'], self.all16_df['HE_BMI'] = self.all14_df['HE_BMI'].apply(BMI_range), self.all16_df['HE_BMI'].apply(BMI_range)\n",
    "        self.pam14_df['sex'], self.pam16_df['sex'], self.all14_df['sex'], self.all16_df['sex'] = self.pam14_df['sex'].apply(Sex_name), self.pam16_df['sex'].apply(Sex_name), self.all14_df['sex'].apply(Sex_name), self.all16_df['sex'].apply(Sex_name)\n",
    "        self.pam14_df['age'], self.pam16_df['age'], self.all14_df['age'], self.all16_df['age'] = self.pam14_df['age'].apply(Age_range), self.pam16_df['age'].apply(Age_range), self.all14_df['age'].apply(Age_range), self.all16_df['age'].apply(Age_range)\n",
    "\n",
    "    def process_data(self):\n",
    "        print('Processing Data')\n",
    "        func = lambda df: df.rename(columns=str.upper)\n",
    "        self.pam14_df, self.pam16_df, self.all14_df, self.all16_df = map(func, [self.pam14_df, self.pam16_df, self.all14_df, self.all16_df])\n",
    "        self.pam_combined = pd.concat([self.pam14_df, self.pam16_df], ignore_index=True)\n",
    "        self.all_combined = pd.concat([self.all14_df, self.all16_df], ignore_index=True)\n",
    "        self.pam_combined.drop('MOD_D', axis=1, inplace=True)\n",
    "        self.pam_combined['ID'] = self.pam_combined['ID'].apply(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)\n",
    "        self.all_combined['ID'] = self.all_combined['ID'].apply(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)\n",
    "\n",
    "    def create_Synthetic_Dataset(self):\n",
    "        print('Creating Synthetic Patients')\n",
    "        self.pam_grouped = self.pam_combined.groupby('ID')\n",
    "        cut = int(len(self.all_combined) * (self.percent / 100))\n",
    "        self.all_combined = self.all_combined.iloc[1:cut]\n",
    "        \n",
    "        # Create an empty list to store pairs of IDs\n",
    "        id_pairs = []\n",
    "        group_names = []\n",
    "        sex_names = []\n",
    "        age_names = []\n",
    "        bmi_names = []\n",
    "        PHQ_value = np.array([])\n",
    "        # Iterate over each group\n",
    "        for name, group in self.all_combined.groupby(['SEX', 'AGE', 'HE_BMI']):\n",
    "            # Get IDs in the group\n",
    "            ids = group['ID'].tolist()\n",
    "            valid_ids = []\n",
    "            for id1 in ids:\n",
    "                try:\n",
    "                    data_participant_1 = self.pam_grouped.get_group(id1)['PAXINTEN'].to_numpy()\n",
    "                    valid_ids.append(id1)\n",
    "                except KeyError:\n",
    "                    pass\n",
    "\n",
    "            for id_1 in valid_ids:\n",
    "                for id_2 in valid_ids:\n",
    "                    if id_1 == id_2: #or (id_2,id_1) in id_pairs:\n",
    "                        pass\n",
    "                    else:\n",
    "                        id_pairs.append((id_1,id_2))\n",
    "                        group_names.append(name[0] + '_' + name[1] + '_' + name[2])\n",
    "                        sex_names.append(name[0])\n",
    "                        age_names.append(name[1])\n",
    "                        bmi_names.append(name[2])\n",
    "\n",
    "                        # PHQ9P1 = all_combined.loc[all_combined['ID'] == id_1, 'BP_PHQ_9'].iloc[0]\n",
    "                        # PHQ9P2 = all_combined.loc[all_combined['ID'] == id_2, 'BP_PHQ_9'].iloc[0]\n",
    "\n",
    "                        PHQSP1 = self.all_combined.loc[self.all_combined['ID'] == id_1, self.depression_feature].iloc[0]\n",
    "                        PHQSP2 = self.all_combined.loc[self.all_combined['ID'] == id_2, self.depression_feature].iloc[0]\n",
    "                        \n",
    "                        value = abs(int(PHQSP1 - PHQSP2))\n",
    "                        PHQ_value = np.append(PHQ_value, value)\n",
    "            \n",
    "        self.id_pairs_df = pd.DataFrame(id_pairs, columns=['ID_1', 'ID_2'])\n",
    "        self.id_pairs_df['group_id'] = group_names\n",
    "        self.id_pairs_df['SEX'] = sex_names\n",
    "        self.id_pairs_df['AGE'] = age_names\n",
    "        self.id_pairs_df['HE_BMI'] = bmi_names\n",
    "        self.id_pairs_df['ID_COMBINED'] = self.id_pairs_df['ID_1'] + self.id_pairs_df['ID_2']\n",
    "        self.id_pairs_df['d_PHQ'] = PHQ_value\n",
    "        self.id_pairs_df['Depression'] = (self.id_pairs_df['d_PHQ'] >= self.threshold).astype(int)\n",
    "\n",
    "    def calculate_actigraphy(self):\n",
    "        print('Calculating Actigraphy Data from Synthetic Patients')\n",
    "        pam_synthetic = pd.DataFrame(columns=['ID','ACTIGRAPHY_DATA'], dtype = object)\n",
    "        synthetic_array = np.zeros((self.id_pairs_df.shape[0], 10080)) # 10080 number of samples for a single patient\n",
    "        id_combined = []\n",
    "        number = 0\n",
    "        for index,synthetic_patient in self.id_pairs_df.iterrows():\n",
    "            \n",
    "            data_participant_1 = self.pam_grouped.get_group(synthetic_patient['ID_1'])['PAXINTEN'].to_numpy()\n",
    "            data_participant_2 = self.pam_grouped.get_group(synthetic_patient['ID_2'])['PAXINTEN'].to_numpy()\n",
    "\n",
    "            op_func = operator_map[self.operator]\n",
    "            result = op_func(data_participant_1, data_participant_2)\n",
    "            \n",
    "            synthetic_array[number] = np.abs(result/2)\n",
    "\n",
    "            id_combined.append(synthetic_patient['ID_1'] + synthetic_patient['ID_2'])\n",
    "            logging.info(f\"Participant_1 {synthetic_patient['ID_1']} and Participant_2 {synthetic_patient['ID_2']} added with {synthetic_array[number]}\")\n",
    "            number += 1\n",
    "            \n",
    "        pam_synthetic['ID'] = id_combined\n",
    "        mask = []\n",
    "        for row in range(synthetic_array.shape[0]):\n",
    "            max_value = np.max(synthetic_array[row, :])\n",
    "            if max_value == 0 or max_value == 0.0:\n",
    "                mask.append(row)\n",
    "        synthetic_array = np.delete(synthetic_array, mask, axis=0)\n",
    "\n",
    "        for row in range(synthetic_array.shape[0]):\n",
    "            pam_synthetic.at[row, 'ACTIGRAPHY_DATA'] = synthetic_array[row]\n",
    "        self.id_pairs_df['ACTIGRAPHY_DATA'] = pam_synthetic['ACTIGRAPHY_DATA']\n",
    "\n",
    "    def plot_data(self, index):\n",
    "        plt.plot(self.id_pairs_df['ACTIGRAPHY_DATA'].iloc[index])\n",
    "\n",
    "    def save_data(self, path):\n",
    "        print(f'Saving Data into {path}')\n",
    "        self.id_pairs_df.to_csv(path, index=False)\n",
    "\n",
    "    def dataset_oversample(self):\n",
    "        if self.depression_feature == 'BP_PHQ_9':\n",
    "            mean_count = int(self.id_pairs_df.groupby('d_PHQ').size().mean()/10) \n",
    "            # Apply the oversampling function to each group\n",
    "            self.id_pairs_df = self.id_pairs_df.groupby('d_PHQ').apply(lambda x: sampling(x, mean_count, self.depression_feature)).reset_index(drop=True)\n",
    "        elif self.depression_feature == 'MH_PHQ_S':\n",
    "            mean_count = int(self.id_pairs_df.groupby('d_PHQ').size().mean()/2) \n",
    "            # Apply the oversampling function to each group\n",
    "            self.id_pairs_df = self.id_pairs_df.groupby('d_PHQ').apply(lambda x: sampling(x, mean_count, self.depression_feature)).reset_index(drop=True)\n",
    "    \n",
    "    def compute_features(self):\n",
    "        feature_list = []\n",
    "        for index,participant in self.id_pairs_df.iterrows():\n",
    "            features = compute_features(participant['ACTIGRAPHY_DATA'])\n",
    "            feature_list.append(features)\n",
    "\n",
    "        feature_list = np.array(feature_list, dtype=np.float32)\n",
    "\n",
    "        for i in range(feature_list.shape[1]):\n",
    "            self.id_pairs_df[f'FEATURE_{i}'] = feature_list[:, i]\n",
    "       \n",
    "    def remove_actigraphy(self):\n",
    "        self.id_pairs_df.drop('ACTIGRAPHY_DATA', axis=1, inplace=True)\n",
    "\n",
    "    def particicipant_distribution(self, before_sampling = True, sampler: int = 1):\n",
    "        grouped = self.id_pairs_df.groupby('d_PHQ')\n",
    "        grouped_counts = []\n",
    "        grouped_names = []\n",
    "        for name, group in grouped:\n",
    "            grouped_counts.append(group.shape[0])\n",
    "            grouped_names.append(name)\n",
    "        # Plotting the counts\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(grouped_names, grouped_counts, color='skyblue')\n",
    "\n",
    "        # Adding titles and labels\n",
    "        plt.title('Counts per Group for d_phq')\n",
    "        plt.xlabel('Participant Group')\n",
    "        plt.ylabel('Count')\n",
    "\n",
    "        if before_sampling:\n",
    "            plt.savefig(f'data/Participant_Distribution_before_sampling_{self.depression_feature}_{sampler}.png')\n",
    "        else:\n",
    "            plt.savefig(f'data/Participant_Distribution_after_sampling_{self.depression_feature}_{sampler}.png')\n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "        \n",
    "    def dataset_oversample_v2(self):\n",
    "        if self.depression_feature == 'BP_PHQ_9':\n",
    "            mean_count = int(self.id_pairs_df.groupby('d_PHQ').size().mean()/10) \n",
    "            # Apply the oversampling function to each group\n",
    "            self.id_pairs_df = self.id_pairs_df.groupby('d_PHQ').apply(lambda x: sampling_v2(x, mean_count)).reset_index(drop=True)\n",
    "        elif self.depression_feature == 'MH_PHQ_S':\n",
    "            mean_count = int(self.id_pairs_df.groupby('d_PHQ').size().mean()/2) \n",
    "            # Apply the oversampling function to each group\n",
    "            self.id_pairs_df = self.id_pairs_df.groupby('d_PHQ').apply(lambda x: sampling_v2(x, mean_count)).reset_index(drop=True)\n",
    "\n",
    "        return True\n",
    "    \n",
    "    def dataset_oversample_v3(self):\n",
    "        number_groups = len(self.id_pairs_df.groupby('d_PHQ').size())\n",
    "        number_without_0 = self.id_pairs_df.groupby('d_PHQ').size()\n",
    "        number_without_0 = number_without_0[number_without_0.index != 0].sum()\n",
    "        self.id_pairs_df = self.id_pairs_df.groupby('d_PHQ').apply(lambda group: sampling_v3(group, number_groups, number_without_0)).reset_index(drop=True)\n",
    "\n",
    "        return True\n",
    "    \n",
    "sample_method = False\n",
    "sampler = 3\n",
    "\n",
    "Dataset = Synthetic_Patient_Dataset(threshold = 10, actigraphy_data_operator = '-', depression_classifier_feature = 'MH_PHQ_S', percent_of_dataset = 100)\n",
    "Dataset.load_data(path_all='ALL/', path_pam='PAM/')\n",
    "Dataset.remove_features()\n",
    "Dataset.create_intervalls()\n",
    "Dataset.process_data()\n",
    "Dataset.create_Synthetic_Dataset()\n",
    "Dataset.calculate_actigraphy()\n",
    "Dataset.compute_features()\n",
    "Dataset.remove_actigraphy()\n",
    "\n",
    "if sampler == 1:\n",
    "    Dataset.particicipant_distribution()\n",
    "    Dataset.dataset_oversample()\n",
    "    Dataset.particicipant_distribution(before_sampling=False)\n",
    "elif sampler == 2:\n",
    "    Dataset.particicipant_distribution(sampler=2)\n",
    "    sample_method = Dataset.dataset_oversample_v2()\n",
    "    Dataset.particicipant_distribution(before_sampling=False, sampler=2)\n",
    "elif sampler == 3:\n",
    "    Dataset.particicipant_distribution(sampler=3)\n",
    "    sample_method = Dataset.dataset_oversample_v3()\n",
    "    Dataset.particicipant_distribution(before_sampling=False, sampler=3)\n",
    "\n",
    "print_information(Dataset.id_pairs_df)\n",
    "\n",
    "Dataset.save_data(f'data/Threshold_{Dataset.threshold}_Operator_{Dataset.operator}_Depressionfeature_{Dataset.depression_feature}_PercentofDataset_{Dataset.percent}_v_{sampler}.csv')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
